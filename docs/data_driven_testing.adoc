= Data Driven Testing
include::include.adoc[]

Oftentimes, it is useful to exercise the same test code multiple times, with varying inputs and expected results.
Spock's data driven testing support makes this a first class feature.

== Introduction

Suppose we want to specify the behavior of the `Math.max` method:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/v1/MathSpec.groovy[tag=example]
----

Although this approach is fine in simple cases like this one, it has some potential drawbacks:

* Code and data are mixed and cannot easily be changed independently
* Data cannot easily be auto-generated or fetched from external sources
* In order to exercise the same code multiple times, it either has to be duplicated or extracted into a separate method
* In case of a failure, it may not be immediately clear which inputs caused the failure
* Exercising the same code multiple times does not benefit from the same isolation as executing separate methods does

Spock's data-driven testing support tries to address these concerns. To get started, let's refactor above code into a
data-driven feature method. First, we introduce three method parameters (called _data variables_) that replace the
hard-coded integer values:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/v2/MathSpec.groovy[tag=example-a]
    ...
include::{sourcedir}/datadriven/v2/MathSpec.groovy[tag=example-b]
----

We have finished the test logic, but still need to supply the data values to be used. This is done in a `where:` block,
which always comes at the end of the method. In the simplest (and most common) case, the `where:` block holds a _data table_.

[[data-tables]]
== Data Tables

Data tables are a convenient way to exercise a feature method with a fixed set of data values:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/v2/MathSpec.groovy[tag=example]
----

The first line of the table, called the _table header_, declares the data variables. The subsequent lines, called
_table rows_, hold the corresponding values. For each row, the feature method will get executed once; we call this an
_iteration_ of the method. If an iteration fails, the remaining iterations will nevertheless be executed. All
failures will be reported.

Data tables must have at least two columns. A single-column table can be written as:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/DataSpec.groovy[tag=single-column]
----

== Isolated Execution of Iterations

Iterations are isolated from each other in the same way as separate feature methods. Each iteration gets its own instance
of the specification class, and the `setup` and `cleanup` methods will be called before and after each iteration,
respectively.

== Sharing of Objects between Iterations

In order to share an object between iterations, it has to be kept in a `@Shared` or static field.

NOTE: Only `@Shared` and static variables can be accessed from within a `where:` block.

Note that such objects will also be shared with other methods. There is currently no good way to share an object
just between iterations of the same method. If you consider this a problem, consider putting each method into a separate
spec, all of which can be kept in the same file. This achieves better isolation at the cost of some boilerplate code.

== Syntactic Variations

The previous code can be tweaked in a few ways. First, since the `where:` block already declares all data variables, the
method parameters can be omitted.footnote:[The idea behind allowing method parameters is to enable better IDE support.
However, recent versions of IntelliJ IDEA recognize data variables automatically, and even infer their types from the
values contained in the data table.]
Second, inputs and expected outputs can be separated with a double pipe symbol (`||`) to visually set them apart.
With this, the code becomes:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/v3/MathSpec.groovy[tag=example]
----

== Reporting of Failures

Let's assume that our implementation of the `max` method has a flaw, and one of the iterations fails:

----
maximum of two numbers [a: 1, b: 3, c: 3, #0]   PASSED
maximum of two numbers [a: 7, b: 4, c: 7, #1]   FAILED

Condition not satisfied:

Math.max(a, b) == c
|    |   |  |  |  |
|    |   7  4  |  7
|    42        false
class java.lang.Math

maximum of two numbers [a: 0, b: 0, c: 0, #2]   PASSED
----

The obvious question is: Which iteration failed, and what are its data values? In our example, it isn't hard to figure
out that it's the second iteration (with index 1) that failed even from the rich condition rendering. At other times
this can be more difficult or even impossible.footnote:[For example, a feature method could use data variables in its
`given:` block, but not in any conditions.] In any case, Spock makes it loud and clear which iteration failed, rather
than just reporting the failure. Iterations of a feature method are by default unrolled with a rich naming pattern.
This pattern can also be configured as documented at <<Unrolled Iteration Names>> or the unrolling can be disabled
like described in the following section.

== Method Uprolling and Unrolling

A method annotated with `@Rollup` will have its iterations not reported independently but only aggregated within the
feature. This can for example be used if you produce many test cases from calculations or if you use external data
like the contents of a database as test data and do not want the test count to vary:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/v4/MathSpec.groovy[tag=example]
  ...
----

Note that up- and unrolling has no effect on how the method gets executed; it is only an alternation in reporting.
Depending on the execution environment, the output will look something like:

----
maximum of two numbers   FAILED

Condition not satisfied:

Math.max(a, b) == c
|    |   |  |  |  |
|    |   7  4  |  7
|    42        false
class java.lang.Math
----

The `@Rollup` annotation can also be placed on a spec.
This has the same effect as placing it on each data-driven feature method of the spec that does not have an
`@Unroll` annotation.

Alternatively the system property `spock.globalRollup` can be set to `true` to uproll all features automatically unless
they are annotated with `@Unroll` or are contained in an ``@Unroll``ed spec and thus reinstate the pre Spock 2.0
behavior where this was the default.

It is illegal to annotate a spec or a feature with both the `@Unroll` and the `@Rollup` annotation and if detected
this will cause an exception to be thrown.

---

So to summarize:

A feature will be uprolled

- if the method is annotated with `@Rollup`
- if the method is not annotated with `@Unroll` and the spec is annotated with `@Rollup`
- if neither the method nor the spec is annotated with `@Unroll`
  and the system property `spock.globalRollup` is set to `true`

A feature will be unrolled

- if the method is annotated with `@Unroll`
- if the method is not annotated with `@Rollup` and the spec is annotated with `@Unroll`
- if neither the method nor the spec is annotated with `@Rollup`
  and the system property `spock.globalRollup` is not set or set to anything but `true`


== Data Pipes

Data tables aren't the only way to supply values to data variables. In fact, a data table is just syntactic sugar for
one or more _data pipes_:

[source,groovy,indent=0]
----
    ...
include::{sourcedir}/datadriven/DataSpec.groovy[tag=data-pipes]
----

A data pipe, indicated by the left-shift (`<<`) operator, connects a data variable to a _data provider_. The data
provider holds all values for the variable, one per iteration. Any object that Groovy knows how to iterate over can be
used as a data provider. This includes objects of type `Collection`, `String`, `Iterable`, and objects implementing the
`Iterable` contract. Data providers don't necessarily have to _be_ the data (as in the case of a `Collection`);
they can fetch data from external sources like text files, databases and spreadsheets, or generate data randomly.
Data providers are queried for their next value only when needed (before the next iteration).

== Multi-Variable Data Pipes

If a data provider returns multiple values per iteration (as an object that Groovy knows how to iterate over),
it can be connected to multiple data variables simultaneously. The syntax is somewhat similar to Groovy multi-assignment
but uses brackets instead of parentheses on the left-hand side:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/DataSpec.groovy[tag=datasource]

include::{sourcedir}/datadriven/DataSpec.groovy[tag=sql-data-pipe]
----

Data values that aren't of interest can be ignored with an underscore (`_`):

[source,groovy,indent=0]
----
    ...
include::{sourcedir}/datadriven/DataSpec.groovy[tag=sql-data-pipe-with-underscore]
----

== Data Variable Assignment

A data variable can be directly assigned a value:

[source,groovy,indent=0]
----
    ...
include::{sourcedir}/datadriven/DataSpec.groovy[tag=data-variable-assignment]
----

Assignments are re-evaluated for every iteration. As already shown above, the right-hand side of an assignment may refer
to other data variables:

[source,groovy,indent=0]
----
    ...
include::{sourcedir}/datadriven/DataSpec.groovy[tag=sql-data-variable-assignment]
----

== Combining Data Tables, Data Pipes, and Variable Assignments

Data tables, data pipes, and variable assignments can be combined as needed:

[source,groovy,indent=0]
----
    ...
include::{sourcedir}/datadriven/DataSpec.groovy[tag=combined-variable-assignment]
----

== Number of Iterations

The number of iterations depends on how much data is available. Successive executions of the same method can
yield different numbers of iterations. If a data provider runs out of values sooner than its peers, an exception will occur.
Variable assignments don't affect the number of iterations. A `where:` block that only contains assignments yields
exactly one iteration.

== Closing of Data Providers

After all iterations have completed, the zero-argument `close` method is called on all data providers that have
such a method.

== Unrolled Iteration Names

By default the names of unrolled iterations are the name of the feature, plus the data variables and the iteration
index. This will always produce unique names and should enable you to identify easily the failing data variable
combination.

The example at <<Reporting of Failures>> for example shows with `maximum of two numbers [a: 7, b: 4, c: 7, #1]`,
that the second iteration (`#1`) where the data variables have the values `7`, `4` and `7` failed.

With a bit of effort, we can do even better:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/v5/MathSpec.groovy[tag=example]
  ...
----

This method name uses placeholders, denoted by a leading hash sign (`#`), to refer to data variables `a`, `b`, and `c`.
In the output, the placeholders will be replaced with concrete values:

----
maximum of 1 and 3 is 3   PASSED
maximum of 7 and 4 is 7   FAILED

Math.max(a, b) == c
|    |   |  |  |  |
|    |   7  4  |  7
|    42        false
class java.lang.Math

maximum of 0 and 0 is 0   PASSED
----

Now we can tell at a glance that the `max` method failed for inputs `7` and `4`.

An unrolled method name is similar to a Groovy `GString`, except for the following differences:

* Expressions are denoted with `#` instead of `$`, and there is no equivalent for the `${...}` syntax.
* Expressions only support property access and zero-arg method calls.

Given a class `Person` with properties `name` and `age`, and a data variable `person` of type `Person`, the
following are valid method names:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/DataSpec.groovy[tag=unrolled-1]
include::{sourcedir}/datadriven/DataSpec.groovy[tag=unrolled-2]
----

Non-string values (like `#person` above) are converted to Strings according to Groovy semantics.

The following are invalid method names:

[source,groovy]
----
def "#person.name.split(' ')[1]" {  // cannot have method arguments
def "#person.age / 2" {  // cannot use operators
----

If necessary, additional data variables can be introduced to hold more complex expressions:

[source,groovy,indent=0]
----
include::{sourcedir}/datadriven/DataSpec.groovy[tag=unrolled-3a]
    ...
include::{sourcedir}/datadriven/DataSpec.groovy[tag=unrolled-3b]
----

Additionally to the data variables the tokens `#featureName` and `#iterationIndex` are supported.
The former does not make much sense inside an actual feature name, but there are two other places
where an unroll pattern can be defined, where it is more useful.

[source,groovy,indent=0]
----
def "#person is #person.age years old [iterationIndex: #iterationIndex]"() {
----

Alternatively to specifying the unroll pattern as method name, it can be given as parameter
to the `@Unroll` annotation which takes precedence over the method name:

[source,groovy,indent=0]
----
@Unroll("#featureName[#iterationIndex] (#person.name is #person.age years old)")
def "person age should be calculated properly"() {
----

If neither a parameter to the annotation is given, nor the method name contains a `#`,
the system property `spock.globalUnrollPattern` is inspected. If it is set to a non-blank
string, this value is used as unroll pattern. This could for example be set to

- `#featureName` to have all iterations reported with the same name, or
- `#featureName[#iterationIndex]` to have a simply indexed iteration name, or
- `#iterationName` if you make sure that in each data-driven feature you also set
  a data variable called `iterationName` that is then used for reporting

If none of the three described ways is used to set a custom unroll pattern, by default
the feature name is used, suffixed with all data variable names and their values and
finally the iteration index, so the result will be for example
`my feature [x: 1, y: 2, z: 3, #0]`.

TIP: You can set the system property `spock.assertUnrollExpressions` to `true`, to let tests fail that have invalid
     unroll expressions. This can be used to help catch errors during refactoring.
